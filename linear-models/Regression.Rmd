---
title: "Regression"
output: html_notebookd
author: "Zachary Canoot & Gray Simpson"
output:
  html_document:
    df_print: paged
  pdf_document: default
---
# Regression

Using the Hungary Dataset [Weather in Szeged 2006-2016](https://www.kaggle.com/datasets/budincsevity/szeged-weather)
Found on Kaggle. 
Our goal is to see if we can see how other weather factors, such as Wind Speed and Humidity, relate to the difference between Apparent Temperature and Temperature.

Linear Regression is one of many supervised models of Machine Learning that functions by finding a trend in given data, using one or more input parameters to find a line of best fit, though it is not always a straight line. As shown by the name, linear regression models assume that the relationship between relevant attributes is linear. The model will predict coefficients for the effect of each predictor. It has low variance due to its linear nature, but with such an assumption, it will also be very high bias.


### Data Exploration
First, we read the data in, then divide our data up into training and testing.
```{r}
df <- read.csv("weatherHistory.csv")
#Here we'll add the data that we are interested in: difference in Apparent Temp and Temp.
#attr(x=df,which="Temp.Diff") <- df[]
df$Temperature.Diff <- df$Temperature..C. - df$Apparent.Temperature..C.
#We'll also convert some data to factors for ease.
df$Precip.Type <- as.factor(df$Precip.Type)
df$Summary <- as.factor(df$Summary)
str(df)
#Now we'll divide into train and test.
set.seed(8)
trainindex <- sample(1:nrow(df),nrow(df)*.8,replace=FALSE)
train <- df[trainindex,]
test <- df[-trainindex,]
```



Next, we want to explore our training data. 
```{r}
names(df)
dim(df)
head(df)
mean(df$Temperature..C.)
mean(df$Apparent.Temperature..C.)
mean(df$Humidity)
mean(df$Wind.Speed..km.h.)
mean(df$Wind.Bearing..degrees.)
mean(df$Visibility..km.)
mean(df$Loud.Cover)
mean(df$Pressure..millibars)

#Noticing the mean of 0 of df$Loud.Cover, lets check its sum in specific.
sum(df$Loud.Cover)
#And thus we realize this attribute is attributively useless.

#Let's see if we have any NAs.
sum(is.na(df$Formatted.Date))
sum(is.na(df$Summary))
sum(is.na(df$Precip.Type))
sum(is.na(df$Temperature..C.))
sum(is.na(df$Apparent.Temperature..C.))
sum(is.na(df$Humidity))
sum(is.na(df$Wind.Speed..km.h.))
sum(is.na(df$Wind.Bearing..degrees.))
sum(is.na(df$Visibility..km.))
sum(is.na(df$Loud.Cover))
sum(is.na(df$Pressure..millibars))
sum(is.na(df$Daily.Summary))
#Okay, so we don't have any NAs.

#Now, lets see how R would summarize this data. 
summary(df)
summary(df$Summary)

```
One thing we notice is that there is an attribute labeled 'Loud Cover' that all values are 0 in.
...Therefore, this will be an aspect that, by and large, we will ignore.
However, in the summary, we can notice that there is a minimum value of 0 on Pressure, which has an average and max similar to each other. We can assume a 0 is a NA value here. 
The other values that are 0 we can't make assumptions on validity.
If Wind Speed is 0, so will Wind Bearing, we can realize from looking through the data in passing. 
Since there is no place in earth without wind, we can also state that these values aren't accurate. After we look a the data some more, we'll decide how we want to clean them up.
We cannot come to a clear resolution on other attributes.


We'll pull up some graphs to get a better idea of what we have to do, now.
Yellow dots are null precipitation days, green is rain, and blue is snow.
```{r}
cor(df[4:7])
boxplot(df$Temperature.Diff,main="Apparent Temperature")
boxplot(df$Humidity,main="Humidity")
boxplot(df$Wind.Speed..km.h.,main="Wind Speed")
#pairs(df[4:7],main="Temperature, Humidity, and Wind Correlations")
plot(df$Temperature.Diff,df$Wind.Speed..km.h.,pch=21,bg=c("yellow","green","blue")[as.integer(df$Precip.Type)])
plot(df$Temperature.Diff,df$Humidity,pch=21,bg=c("yellow","green","blue")[as.integer(df$Precip.Type)])
plot(df$Temperature.Diff, df$Temperature..C.,pch=21,bg=c("yellow","green","blue")[as.integer(df$Precip.Type)])

```
We can notice that there are some outliers in humidity at the 0 line we'll want to sort out, as well.
We also notice a sort of "knife" shape in the data when being compared, at around 10 degrees Celsius.
By and large, we have such a large amount of data, it's difficult to notice quick correlations, aside from wind speed and temperature when it is snowing/below freezing.
That's why we have Machine Learning, we suppose. 
While we would like to predict the results without the base temperature, we can see that is is very clearly related and helpful.



Now, we'll clean up the data according to what we found. We'll clean up only what is referenced, but we will delete what we are uncertain about, since we have such a large amount of data.
```{r}
df[,6:7][df[,6:7]==0] <- NA
df[,13:13][df[,13:13]==0] <- NA
df <- na.omit(df)
summary(df)
```


Now we'll do some more graphs.
```{r}
cor(df[4:7])
boxplot(df$Temperature.Diff,main="Apparent Temperature")
boxplot(df$Humidity,main="Humidity")
boxplot(df$Wind.Speed..km.h.,main="Wind Speed")
#pairs(df[4:7],main="Temperature, Humidity, and Wind Correlations")
plot(df$Temperature.Diff,df$Wind.Speed..km.h.,pch=21,bg=c("yellow","green","blue")[as.integer(df$Precip.Type)])
plot(df$Temperature.Diff,df$Humidity,pch=21,bg=c("yellow","green","blue")[as.integer(df$Precip.Type)])
plot(df$Temperature.Diff, df$Temperature..C.,pch=21,bg=c("yellow","green","blue")[as.integer(df$Precip.Type)])

```

Before I move on to linear regression, I have one more question before I investigate disparities in the data. 
Since this project has multiple contributors, perhaps there are even more hidden NA's. 
Let's see what the data looks like if we remove data where there is no difference. 
```{r}
diffOnly <- df
diffOnly[,3:4][diffOnly[,3:3]==diffOnly[,4:4]] <- NA
diffOnly <- na.omit(diffOnly)
plot(diffOnly$Temperature.Diff,diffOnly$Wind.Speed..km.h.,pch=21,bg=c("yellow","green","blue")[as.integer(df$Precip.Type)])
plot(diffOnly$Temperature.Diff,diffOnly$Humidity,pch=21,bg=c("yellow","green","blue")[as.integer(df$Precip.Type)])
#These graphs show the overall descriptor of the weather. There are 27 options.
plot(diffOnly$Temperature.Diff,diffOnly$Wind.Speed..km.h.,pch=21,bg=c("white","aquamarine","blue","brown","green","yellow","red","cyan","darkgray","darkgreen","magenta","orange","dodgerblue","forestgreen","gold","sienna","thistle","violet","springgreen","slateblue","wheat","tomato","yellowgreen","tan","lightblue","hotpink","darkred")[as.integer(df$Summary)])
plot(diffOnly$Temperature.Diff,diffOnly$Humidity,pch=21,bg=c("white","aquamarine","blue","brown","green","yellow","red","cyan","darkgray","darkgreen","magenta","orange","dodgerblue","forestgreen","gold","sienna","thistle","violet","springgreen","slateblue","wheat","tomato","yellowgreen","tan","lightblue","hotpink","darkred")[as.integer(df$Summary)])
plot(df$Temperature.Diff, df$Temperature..C.,pch=21,bg=c("white","aquamarine","blue","brown","green","yellow","red","cyan","darkgray","darkgreen","magenta","orange","dodgerblue","forestgreen","gold","sienna","thistle","violet","springgreen","slateblue","wheat","tomato","yellowgreen","tan","lightblue","hotpink","darkred")[as.integer(df$Summary)])

```
This does not seem to have effected data trends very much.
Looking at the data based on Summary does not help us much, but we can notice that the cloud of data that does not seem to have much trend is in the violet(18) and slateblue (20), or rather Mostly Cloudy and Partly Cloudy. There isn't much helpful we can do with that information at this time, however.

Since that seemed to cause no changes, but may have helped clean it up a small amount, let's write it to df.
We'll also clean up the train and test data again.
```{r}
df <- diffOnly
trainindex <- sample(1:nrow(df),nrow(df)*.8,replace=FALSE)
train <- df[trainindex,]
test <- df[-trainindex,]
```


Now, we'll move on to the regression.


### Linear Regression: Simple
Let's start with a linear regression model with one predictor, wind speed, and summarize it. 
```{r}
simplelinreg <- lm(Temperature.Diff~Wind.Speed..km.h.,data=train)
summary(simplelinreg)
```
So, it's better than nothing it seems. The R^2 isn't great, but we can see that there's enough of a correlation to count. We could get a better reading by using the actual temperature, since those are very closely related, but one goal of this is learning to understand how the change in temperature works based on other factors. 

Lets plot the residual errors, and evaluate.
```{r}
par(mfrow=c(2,2))
plot(simplelinreg)
```
[insert explanation of what the residual plot tells us]


### Linear Regression: Multiple
Let's up the complexity, now. We'll build a multiple linear regression model, and see if we can improve the accuracy.
```{r}
multlinreg <- lm(Temperature.Diff~Humidity+Wind.Speed..km.h.+Precip.Type,data=train)
summary(multlinreg)
```
[short explanation of it]

### Linear Regression: Combinations
Now let's go a step even farther. We'll use a combination of predictors, interaction effects, and polynomial regression to see if we can get even more accurate. 
```{r}
combolinreg <- lm(Temperature.Diff~poly(Humidity*Wind.Speed..km.h.)+Precip.Type*Summary,data=train)
summary(combolinreg)
```

### Evaluation 
[long explanation talking about which model was the best, comparing the results, and explaining why we liked which model the best]


### Predictions
Using the three models, we will predict and evaluate using the metric correlation and MSE. 
```{r}
simplepred <- predict(simplelinreg,newdata=test)
simplecor <- cor(simplepred,test$Temperature.Diff)
simplemse <- mean((simplepred-test$Temperature.Diff)^2)
simplermse <- sqrt(simplemse)
multpred <- predict(multlinreg,newdata=test)
multcor <- cor(multpred,test$Temperature.Diff)
multmse <- mean((multpred-test$Temperature.Diff)^2)
multrmse <- sqrt(multmse)
combopred <- predict(combolinreg,newdata=test)
combocor <- cor(combopred,test$Temperature.Diff)
combomse <- mean((combopred-test$Temperature.Diff)^2)
combormse <- sqrt(combomse)
#Output results
print("-------Simple Model-------")
print(paste("Correlation: ", simplecor))
print(paste("MSE: ", simplemse))
print(paste("RMSE: ", simplermse))
print("-------Multiple Model-------")
print(paste("Correlation: ", multcor))
print(paste("MSE: ", multmse))
print(paste("RMSE: ", multrmse))
print("-------Combo Model-------")
print(paste("Correlation: ", combocor))
print(paste("MSE: ", combomse))
print(paste("RMSE: ", combormse))
```
[insert explanation, comparing the results and indicating why we think they happened]



